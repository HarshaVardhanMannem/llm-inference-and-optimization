{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì BERT to DistilBERT Knowledge Distillation Tutorial",
    "",
    "<div style=\"text-align: center; padding: 20px 0; margin-bottom: 30px; border-bottom: 3px solid #667eea;\">",
    "<h1 style=\"color: #667eea; font-size: 2em; margin: 0; font-weight: bold; line-height: 1.2;\">üéì BERT to DistilBERT</h1>",
    "<h2 style=\"color: #764ba2; font-size: 1.5em; margin: 10px 0 0 0; font-weight: 600; line-height: 1.3;\">Knowledge Distillation Tutorial</h2>",
    "</div>",
    "",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; color: white; margin: 20px 0;\">",
    "<h2 style=\"color: white; margin-top: 0; font-size: 1.3em; font-weight: 600;\">üìö Overview</h2>",
    "<p style=\"font-size: 1em; line-height: 1.6; color: white; margin-bottom: 0;\">",
    "This notebook demonstrates <strong>Knowledge Distillation</strong> - a powerful technique to compress a large, accurate model (teacher) into a smaller, faster model (student) while preserving performance.",
    "</p>",
    "</div>",
    "",
    "## üéØ What is Knowledge Distillation?",
    "",
    "<div style=\"background-color: #f8f9fa; padding: 15px; border-left: 4px solid #667eea; margin: 15px 0;\">",
    "<ul style=\"margin: 0; padding-left: 20px; color: #212529;\">",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">üë®‚Äçüè´ Teacher Model</strong>: Large, pre-trained BERT model fine-tuned on SST-2 (sentiment analysis)</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">üë®‚Äçüéì Student Model</strong>: Smaller DistilBERT model that learns from the teacher</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">üéØ Goal</strong>: Transfer the teacher's knowledge to the student, achieving similar accuracy with ~60% fewer parameters</li>",
    "</ul>",
    "</div>",
    "",
    "## üîë Key Concepts",
    "",
    "<div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px; margin: 20px 0;\">",
    "<div style=\"background: #e3f2fd; padding: 15px; border-radius: 8px; border-top: 3px solid #2196f3;\">",
    "<h3 style=\"margin-top: 0; color: #1976d2; font-size: 1.1em; font-weight: 600;\">üå°Ô∏è Temperature Scaling</h3>",
    "<p style=\"margin-bottom: 0; color: #212529; font-size: 0.95em;\">Softens probability distributions to reveal \"dark knowledge\"</p>",
    "</div>",
    "<div style=\"background: #f3e5f5; padding: 15px; border-radius: 8px; border-top: 3px solid #9c27b0;\">",
    "<h3 style=\"margin-top: 0; color: #7b1fa2; font-size: 1.1em; font-weight: 600;\">üìä KL Divergence Loss</h3>",
    "<p style=\"margin-bottom: 0; color: #212529; font-size: 0.95em;\">Measures how well student matches teacher's predictions</p>",
    "</div>",
    "<div style=\"background: #fff3e0; padding: 15px; border-radius: 8px; border-top: 3px solid #ff9800;\">",
    "<h3 style=\"margin-top: 0; color: #e65100; font-size: 1.1em; font-weight: 600;\">‚öñÔ∏è Combined Loss</h3>",
    "<p style=\"margin-bottom: 0; color: #212529; font-size: 0.95em;\">Balances hard labels (ground truth) and soft labels (teacher predictions)</p>",
    "</div>",
    "</div>",
    "",
    "## üìã Notebook Structure",
    "",
    "<div style=\"background-color: #fff; border: 2px solid #e0e0e0; border-radius: 8px; padding: 15px; margin: 15px 0;\">",
    "<ol style=\"margin: 0; padding-left: 20px; color: #212529;\">",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">üîß Setup</strong>: Install dependencies and load models</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">üß† Distillation</strong>: Custom trainer implementing knowledge distillation loss</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">üöÄ Training</strong>: Train student model using teacher's soft predictions</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">üìà Evaluation</strong>: Benchmark teacher vs student performance</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">‚òÅÔ∏è Deployment</strong>: Upload distilled model to Hugging Face Hub</li>",
    "</ol>",
    "</div>",
    "",
    "<div style=\"background-color: #d4edda; border-left: 4px solid #28a745; padding: 12px; margin: 20px 0;\">",
    "<strong style=\"color: #155724;\">üí° Tip:</strong> <span style=\"color: #155724;\">Run cells sequentially to complete the distillation pipeline.</span>",
    "</div>",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PPX1jXodPMb",
    "outputId": "e0dde828-1d61-44cd-db23-1063fd5f4997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate evaluate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqA94pvPt8OQ",
    "outputId": "b5f51016-fba6-4a71-df15-ab9929259d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
      "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.49.0\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes accelerate scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 1: Load Teacher and Student Models",
    "",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 15px; border-radius: 8px; color: white; margin-bottom: 20px;\">",
    "<h3 style=\"color: white; margin-top: 0; font-size: 1.2em; font-weight: 600;\">üë®‚Äçüè´ Teacher Model: BERT-base-uncased (Fine-tuned on SST-2)</h3>",
    "</div>",
    "",
    "<div style=\"background-color: #f5f5f5; padding: 15px; border-radius: 8px; margin: 15px 0;\">",
    "<ul style=\"margin: 0; padding-left: 20px; color: #212529;\">",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Model</strong>: <code style=\"background: #fff; padding: 2px 6px; border-radius: 3px; color: #d32f2f;\">textattack/bert-base-uncased-SST-2</code></li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Parameters</strong>: <span style=\"color: #d32f2f; font-weight: bold;\">~110M parameters</span></li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Purpose</strong>: Pre-trained and fine-tuned on Stanford Sentiment Treebank (SST-2)</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Role</strong>: Provides \"soft labels\" (probability distributions) instead of just hard labels</li>",
    "</ul>",
    "</div>",
    "",
    "<div style=\"background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); padding: 15px; border-radius: 8px; color: white; margin-bottom: 20px;\">",
    "<h3 style=\"color: white; margin-top: 0; font-size: 1.2em; font-weight: 600;\">üë®‚Äçüéì Student Model: DistilBERT-base-uncased</h3>",
    "</div>",
    "",
    "<div style=\"background-color: #f5f5f5; padding: 15px; border-radius: 8px; margin: 15px 0;\">",
    "<ul style=\"margin: 0; padding-left: 20px; color: #212529;\">",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Model</strong>: <code style=\"background: #fff; padding: 2px 6px; border-radius: 3px; color: #388e3c;\">distilbert-base-uncased</code></li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Parameters</strong>: <span style=\"color: #388e3c; font-weight: bold;\">~67M parameters</span> (<span style=\"color: #d32f2f;\">~60% smaller</span>)</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Purpose</strong>: Smaller, faster model that will learn from teacher</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Initialization</strong>: Starts with generic pre-trained weights, not fine-tuned</li>",
    "</ul>",
    "</div>",
    "",
    "### ü§î Why This Pair?",
    "",
    "<div style=\"background-color: #e3f2fd; padding: 15px; border-left: 4px solid #2196f3; margin: 15px 0;\">",
    "<ul style=\"margin: 0; padding-left: 20px; color: #212529;\">",
    "<li>DistilBERT is architecturally similar to BERT but with fewer layers</li>",
    "<li>Both use the same tokenizer, making knowledge transfer easier</li>",
    "<li>Size reduction enables <strong style=\"color: #000;\">faster inference</strong> and <strong style=\"color: #000;\">lower memory usage</strong></li>",
    "</ul>",
    "</div>",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474,
     "referenced_widgets": [
      "703acf4dc4574b858df25b8ae4a709c2",
      "d62acbff2b6342e2882150379155b440",
      "497ee801e667409ca236ca9e41a8e594",
      "d354fae795b746c3b91c6b568f4b692c",
      "f368bfa55b414affbed0d91655dfad24",
      "c314d6398d6d4b7a8b534723ebdaa3b9",
      "8c18616957b24e6788e1d0714eed4afc",
      "2c8ffc5ef9ca468f91a6d48a4fbff71e",
      "241fc028363d4ea6876f36af07b06be1",
      "ea2b2c6355c545a182740a7185fed3e7",
      "7295ebcb5ad84d4eb97bc076bd74b669",
      "ed5d0f1259594268bfd85662bed33349",
      "28e75714e4a84a3f9f4a504b12c62643",
      "7c53ca216fdf4483ad939c3a83db17b2",
      "111cf6c1490c4b02a8abd8ac2e990168",
      "7aabe5e7008042189453ca4f11abf66d",
      "79be173d26b740d0a9b8d16101f8faf6",
      "fd7c820b52ef403b966ebc9136c43b7e",
      "33371b275ff446b08345ce0613e73f4b",
      "12d9f0fe5f964dc08bff32f04a1da830",
      "4c8786bd62e94ed89381c0635a2266e7",
      "5302e89ea952444aa4e0d6f8bb0e518e",
      "c04d8fe275b64fef87f9feb69e313340",
      "7fdc270219a246639ad71e75a274371c",
      "7d19f282d63b48168988ab891e9708ef",
      "d1912fd5eba94585b95e2be2239a1e34",
      "d7e331d258e04bab8d957a95d909127d",
      "a356a24a5bb4456497ca0c03550d447d",
      "d7b36563d9e44341b18e9ce59a012113",
      "77c7505f84da4ef1aa324c3828965630",
      "8a70c2a6992a48b79d66053920f52b20",
      "3860dfad033a4008b6be00a7023a386e",
      "5f0bd3abf08a4bbfa449cfa8ab2dbf35",
      "e69c2e26e616413f9cd6a075d949a50d",
      "64a7d9c4059248358fef45f7294ca403",
      "2e0fc57f28694d6786f275b7af9bf273",
      "abe027ea16fd46258ca9d4c89719d4e7",
      "ad09826c35c549be9bbc511f305e3f01",
      "1163fd0cfa444483b5ddc2909f65aa61",
      "60bbb1eb3d1d4e16a5ec6b73e768a914",
      "f18e089f4a2b41bf81b0b65dd7e24858",
      "dca2ba1a116a4cf2ac88f2dd642f8a4b",
      "b9e913926fcc41eda169c14d70c7793f",
      "c7113ae167c54b2fa98ed47181d33edd",
      "db3cac0c81fa493a97efca2f070d3131",
      "c5e2c7a98ca844d5aac4e0696ac9ea3e",
      "2a395beebebf40b1b01688d15592c293",
      "f39abf57b3394f42a944a74e467f16df",
      "c7a5f133e9df498281583ed254c46606",
      "a80608260b314e7fa10639fe947d7006",
      "227f3a4f664246058dfd4018e53e5b1c",
      "69ae98bbda2847d185481f4f5370ad8b",
      "ec202fc3ed094fa7b481d7e361d42d52",
      "1c0d69e32ee5457da464425b10ca9d9c",
      "f2cd689a7c7448888889d9e2a9de43c6",
      "84d165a069c441a29b480a7ea07c5454",
      "5379716e547a462c9fab7107052b004d",
      "31f61ada833f4f0ea7c38b8960db668b",
      "15f3d3b0a02e4cdd9f074733714e4458",
      "016ed9fb1ff1467493d967cbd9e31abe",
      "fa92e05e8bb64d4f87b9d7478cc4cbfe",
      "2233db36326a4ab2b806ce6e8764ad38",
      "5d2342a2a69d426788618d70a8e497bf",
      "9a4ab81797b644d4ab7db58cb8b37850",
      "ab39870c702f4acbb3115441bf06a6a6",
      "fc16913f587d475fab7294ca7fe60b1d",
      "d503c3f3892c4716926f80f8fbedbd40",
      "987d3f6f93144fea89ede18b259d2544",
      "072096eca4c3475c8794b1718deba122",
      "7282d429bed94b6193b5791972afac63",
      "e4bcb59ab2df495591b50eb30c2b491c",
      "6624dd9efa6f4f7390a3b43e89c8f9a0",
      "87937c85197542ada3e4c74d3374fd78",
      "0177fffaadfc470795f50c3ee74da3f5",
      "4c4a6cba108443938e5613da489b6640",
      "a9e30fa390f543ae8ef9663f41eab5bf",
      "1fb33afde2154948982ec584adf98044",
      "997eff9b35d24890862dc8746a63beac",
      "b16daf6a0b6a440abaf63d91f17b9322",
      "364a379e961b4574b81ebb4fb1e49f6f",
      "7f600c36646e4d838322017fcc7cab48",
      "ffc4001f96f14d58a32c467ad2fe2233",
      "6610104517df486fa0f59de8768e9545",
      "9c13740d591147ed918900369c7ba4fa",
      "6dbcda9b9b12475081b1e432df30e367",
      "7e8ebae6090c4318b440d39d3ad17895",
      "4d19be5d594d4a35a4d5c5d4d9f25cce",
      "7137f90e87ee4f24a492132afa6874d2"
     ]
    },
    "id": "M-wAYZGAdSuH",
    "outputId": "4ee640e3-3a72-476c-c92a-1db5d448d00d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher parameters: 109,483,778\n",
      "Student parameters: 66,955,010\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 1. Load the Teacher (Already fine-tuned on SST-2)\n",
    "teacher_id = \"textattack/bert-base-uncased-SST-2\"\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_id)\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_id)\n",
    "\n",
    "# 2. Load the Student (Smaller, generic DistilBERT)\n",
    "student_id = \"distilbert-base-uncased\"\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    student_id,\n",
    "    num_labels=2,\n",
    "    id2label=teacher_model.config.id2label,\n",
    "    label2id=teacher_model.config.label2id\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)\n",
    "\n",
    "print(f\"Teacher parameters: {teacher_model.num_parameters():,}\")\n",
    "print(f\"Student parameters: {student_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 2: Custom Distillation Trainer",
    "",
    "<div style=\"background-color: #fff3e0; padding: 20px; border-radius: 10px; border: 2px solid #ff9800; margin: 20px 0;\">",
    "<h3 style=\"color: #e65100; margin-top: 0; font-size: 1.3em; font-weight: 600;\">DistillationTrainer Class</h3>",
    "<p style=\"color: #212529; margin-bottom: 0;\">This custom trainer extends Hugging Face's <code style=\"background: #f5f5f5; padding: 2px 6px; border-radius: 3px; color: #000;\">Trainer</code> to implement knowledge distillation.</p>",
    "</div>",
    "",
    "### üîß Key Components:",
    "",
    "<div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 15px; margin: 20px 0;\">",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 15px; border-radius: 8px; color: white;\">",
    "<h4 style=\"color: white; margin-top: 0; font-size: 1.1em; font-weight: 600;\">üå°Ô∏è Temperature Scaling</h4>",
    "<p style=\"margin-bottom: 5px; color: white; font-size: 0.9em;\"><code style=\"background: rgba(255,255,255,0.2); padding: 2px 6px; border-radius: 3px; color: white;\">temperature=2.0</code></p>",
    "<ul style=\"margin: 0; padding-left: 20px; font-size: 0.9em; color: white;\">",
    "<li>Divides logits by temperature before softmax</li>",
    "<li>Higher temperature = softer probability distributions</li>",
    "<li>Reveals relationships between classes</li>",
    "</ul>",
    "</div>",
    "",
    "<div style=\"background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); padding: 15px; border-radius: 8px; color: white;\">",
    "<h4 style=\"color: white; margin-top: 0; font-size: 1.1em; font-weight: 600;\">üìä KL Divergence Loss</h4>",
    "<p style=\"margin-bottom: 5px; color: white; font-size: 0.9em;\"><code style=\"background: rgba(255,255,255,0.2); padding: 2px 6px; border-radius: 3px; color: white;\">loss_distill</code></p>",
    "<ul style=\"margin: 0; padding-left: 20px; font-size: 0.9em; color: white;\">",
    "<li>Measures student-teacher distribution match</li>",
    "<li>Formula: <code style=\"background: rgba(255,255,255,0.2); color: white;\">KL(student_softmax || teacher_softmax)</code></li>",
    "<li>Multiplied by <code style=\"background: rgba(255,255,255,0.2); color: white;\">temperature¬≤</code> to scale back</li>",
    "</ul>",
    "</div>",
    "",
    "<div style=\"background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); padding: 15px; border-radius: 8px; color: white;\">",
    "<h4 style=\"color: white; margin-top: 0; font-size: 1.1em; font-weight: 600;\">‚öñÔ∏è Combined Loss</h4>",
    "<p style=\"margin-bottom: 5px; color: white; font-size: 0.9em;\"><code style=\"background: rgba(255,255,255,0.2); padding: 2px 6px; border-radius: 3px; color: white;\">alpha=0.5</code></p>",
    "<ul style=\"margin: 0; padding-left: 20px; font-size: 0.9em; color: white;\">",
    "<li><code style=\"background: rgba(255,255,255,0.2); color: white;\">loss = Œ± √ó loss_ce + (1-Œ±) √ó loss_distill</code></li>",
    "<li>Balances hard labels vs teacher predictions</li>",
    "<li>Typical range: 0.3-0.7</li>",
    "</ul>",
    "</div>",
    "</div>",
    "",
    "<div style=\"background-color: #d4edda; border-left: 4px solid #28a745; padding: 12px; margin: 20px 0;\">",
    "<strong style=\"color: #155724;\">üîí Why Freeze Teacher?</strong> <span style=\"color: #155724;\">Teacher weights are frozen - only student learns. Teacher provides guidance without being modified.</span>",
    "</div>",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aB_wRQoCdU1N"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        # Freeze teacher weights (we only learn from them, we don't update them)\n",
    "        self.teacher_model.eval()\n",
    "        self.teacher_model.requires_grad_(False)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # 1. Forward pass student\n",
    "        outputs_student = model(**inputs)\n",
    "        student_logits = outputs_student.logits\n",
    "\n",
    "        # 2. Forward pass teacher (with no gradient tracking for efficiency)\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher_model(**inputs)\n",
    "            teacher_logits = outputs_teacher.logits\n",
    "\n",
    "        # 3. Calculate \"Dark Knowledge\" Loss (KL Divergence)\n",
    "        # We soften the logits using the Temperature (T)\n",
    "        loss_distill = F.kl_div(\n",
    "            F.log_softmax(student_logits / self.temperature, dim=1),\n",
    "            F.softmax(teacher_logits / self.temperature, dim=1),\n",
    "            reduction='batchmean',\n",
    "        ) * (self.temperature ** 2)\n",
    "\n",
    "        # 4. Calculate Standard Loss (Cross Entropy with Ground Truth)\n",
    "        # Note: 'labels' are automatically handled by the model's internal loss calculation if present\n",
    "        loss_ce = outputs_student.loss\n",
    "\n",
    "        # 5. Combine them\n",
    "        # alpha controls how much we trust the hard labels vs the teacher\n",
    "        loss = (self.alpha * loss_ce) + ((1 - self.alpha) * loss_distill)\n",
    "\n",
    "        return (loss, outputs_student) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Training with Knowledge Distillation",
    "",
    "<div style=\"background: linear-gradient(135deg, #fa709a 0%, #fee140 100%); padding: 20px; border-radius: 10px; margin: 20px 0;\">",
    "<h3 style=\"margin-top: 0; color: #212529; font-size: 1.3em; font-weight: 600;\">üìä Dataset: SST-2 (Stanford Sentiment Treebank)</h3>",
    "<ul style=\"margin: 0; padding-left: 20px; color: #212529;\">",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Task</strong>: Binary sentiment classification (positive/negative)</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Samples</strong>: ~67K training, 872 validation, 1.8K test</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Format</strong>: Movie review sentences with sentiment labels</li>",
    "</ul>",
    "</div>",
    "",
    "### üîÑ Training Process:",
    "",
    "<div style=\"background-color: #f5f5f5; padding: 15px; border-radius: 8px; margin: 15px 0;\">",
    "<ol style=\"margin: 0; padding-left: 20px; color: #212529;\">",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Tokenization</strong>: Convert sentences to token IDs (max_length=128)</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Forward Pass</strong>: ",
    "   <ul style=\"color: #212529;\">",
    "   <li>Student processes inputs ‚Üí student logits</li>",
    "   <li>Teacher processes inputs (no gradients) ‚Üí teacher logits</li>",
    "   </ul>",
    "</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Loss Calculation</strong>:",
    "   <ul style=\"color: #212529;\">",
    "   <li>Apply temperature scaling to both logits</li>",
    "   <li>Compute KL divergence between distributions</li>",
    "   <li>Compute cross-entropy with ground truth</li>",
    "   <li>Combine losses with alpha weighting</li>",
    "   </ul>",
    "</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">Backward Pass</strong>: Update only student model weights</li>",
    "</ol>",
    "</div>",
    "",
    "### ‚öôÔ∏è Training Configuration:",
    "",
    "<div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 10px; margin: 20px 0;\">",
    "<div style=\"background: #e3f2fd; padding: 10px; border-radius: 5px; text-align: center;\">",
    "<div style=\"font-size: 1.5em; font-weight: bold; color: #1976d2;\">3</div>",
    "<div style=\"font-size: 0.9em; color: #212529;\">Epochs</div>",
    "</div>",
    "<div style=\"background: #f3e5f5; padding: 10px; border-radius: 5px; text-align: center;\">",
    "<div style=\"font-size: 1.5em; font-weight: bold; color: #7b1fa2;\">32</div>",
    "<div style=\"font-size: 0.9em; color: #212529;\">Batch Size</div>",
    "</div>",
    "<div style=\"background: #fff3e0; padding: 10px; border-radius: 5px; text-align: center;\">",
    "<div style=\"font-size: 1.5em; font-weight: bold; color: #e65100;\">2e-5</div>",
    "<div style=\"font-size: 0.9em; color: #212529;\">Learning Rate</div>",
    "</div>",
    "<div style=\"background: #e8f5e9; padding: 10px; border-radius: 5px; text-align: center;\">",
    "<div style=\"font-size: 1.5em; font-weight: bold; color: #388e3c;\">2.0</div>",
    "<div style=\"font-size: 0.9em; color: #212529;\">Temperature</div>",
    "</div>",
    "</div>",
    "",
    "### üìà Expected Results:",
    "",
    "<div style=\"background-color: #d4edda; border-left: 4px solid #28a745; padding: 15px; margin: 20px 0;\">",
    "<ul style=\"margin: 0; padding-left: 20px; color: #155724;\">",
    "<li>Student should achieve <strong style=\"color: #155724;\">~90-92% accuracy</strong> (close to teacher's ~93%)</li>",
    "<li>Model size: <strong style=\"color: #155724;\">~67M params</strong> vs teacher's ~110M params</li>",
    "<li>Inference speed: <strong style=\"color: #155724;\">~2x faster</strong> than teacher</li>",
    "</ul>",
    "</div>",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698,
     "referenced_widgets": [
      "bde24b46e9994241a65aa3f0dc602983",
      "0e05344629a34df79df2e294b21cf768",
      "5e36b1a43a474aa7a12d88c93da91f29",
      "7ff707c37647467ea9552dbb77943f46",
      "79a6193e6a154c1699e6f5907b07583a",
      "ac5dc1d0997e471e81c9dcafc2ad5cab",
      "32b82f11ef784662ba415968de95048f",
      "4c7dffd006584e9f94bd6a749c53223c",
      "4cfa4f11d34d466195360582ffd35696",
      "8f4833f1fa4948a292087fd556f7f27a",
      "6700d4974b364749809f786c19873ae6",
      "8ae3324db171439699afc66fa0d39151",
      "b72f063f879a423abd829ff43528bb5a",
      "b9d70e37ab1d48c0b796f7a1eeb1c53e",
      "a97e2b3e9dfb496082b0e632ee69eaaa",
      "75f0ebd134ed44bf9f87944cd141da58",
      "fff94bf032af489fb792c1d3bf45a6d5",
      "0c56bf9f4cbe41ef872f6aaf4dd569a1",
      "17fc70327ab54de989b3e537d084edf3",
      "c8e08186754842b88e6cccae7b5eb088",
      "e68bcb2e9a414e6fac816b42927308fe",
      "9e94034e8d7e419a9115c54bd7bad68b",
      "9fa6077ac064428197849475061e8d57",
      "d2630f936bce4faea3b26965836f7ede",
      "c1ce570b6fb04bc1807e5a2bd2566568",
      "7eae62dd71e048adb0ff238f8b2efd51",
      "be46309cb2fd475eaef3cb469c06504a",
      "dbc16bd084fb442f9560b41565946d89",
      "96fd5b7268fb45f2a6bf656ee2da8140",
      "1264e9d148b940e3a5d9ce4c29f12e24",
      "1ff16ed461e848f7b7b1be2a793ac182",
      "c9f6d36f1bc84bb6b2eda00592f98d92",
      "bacf0421270945ab9e3ade0d2c6db147"
     ]
    },
    "id": "VGcFSUCTdkMs",
    "outputId": "9243b5ad-b547-484d-e30c-e11eb3a2036d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3687500800.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharsha90145\u001b[0m (\u001b[33mharsha90145-university-of-alabama-at-birmingham\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251229_001829-gmtievmw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harsha90145-university-of-alabama-at-birmingham/huggingface/runs/gmtievmw' target=\"_blank\">good-lion-16</a></strong> to <a href='https://wandb.ai/harsha90145-university-of-alabama-at-birmingham/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harsha90145-university-of-alabama-at-birmingham/huggingface' target=\"_blank\">https://wandb.ai/harsha90145-university-of-alabama-at-birmingham/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harsha90145-university-of-alabama-at-birmingham/huggingface/runs/gmtievmw' target=\"_blank\">https://wandb.ai/harsha90145-university-of-alabama-at-birmingham/huggingface/runs/gmtievmw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6315' max='6315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6315/6315 55:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.263400</td>\n",
       "      <td>0.346283</td>\n",
       "      <td>0.900229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.166400</td>\n",
       "      <td>0.335335</td>\n",
       "      <td>0.900229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>0.333426</td>\n",
       "      <td>0.903670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6315, training_loss=0.24634105479632307, metrics={'train_runtime': 3400.9809, 'train_samples_per_second': 59.408, 'train_steps_per_second': 1.857, 'total_flos': 6691160124062208.0, 'train_loss': 0.24634105479632307, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load Data\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return teacher_tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define Metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilled-bert-sst2\",\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Initialize our Custom Trainer\n",
    "trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    teacher_model=teacher_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=teacher_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    temperature=4.0,  # Soften the probability distribution\n",
    "    alpha=0.5         # 50% Hard Labels, 50% Teacher Knowledge\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8sTnrp3DsPhd"
   },
   "outputs": [],
   "source": [
    "student_model.save_pretrained(\"./distilled-bert-sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yleq2PXUrgCg",
    "outputId": "82230611-c982-4c31-d870-ac323ca63922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'The movie was overly long and the plot was confusing.'\n",
      " -> Label: NEGATIVE | Confidence: 99.89%\n",
      "\n",
      "Input: 'An absolute masterpiece that I would watch again in a heartbeat.'\n",
      " -> Label: POSITIVE | Confidence: 99.97%\n",
      "\n",
      "Input: 'It was okay, not great but not terrible either.'\n",
      " -> Label: POSITIVE | Confidence: 94.06%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict(text, model, tokenizer, device=\"cpu\"):\n",
    "    # 1. Prepare the model\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 2. Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "\n",
    "    # --- THE FIX IS HERE ---\n",
    "    # BERT tokenizers create 'token_type_ids', but DistilBERT crashes if it sees them.\n",
    "    # We simply remove them from the dictionary if they exist.\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "\n",
    "    # Move remaining inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # 3. Run Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # 4. Convert to Probabilities\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "    # 5. Format Output\n",
    "    prediction_id = torch.argmax(probabilities, dim=1).item()\n",
    "    confidence = probabilities[0][prediction_id].item()\n",
    "\n",
    "    label_map = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "    label_name = label_map[prediction_id]\n",
    "\n",
    "    return f\"Label: {label_name} | Confidence: {confidence:.2%}\"\n",
    "\n",
    "# --- Re-run Test Cases ---\n",
    "# Make sure model is on CPU for this quick test\n",
    "student_model.to(\"cpu\")\n",
    "\n",
    "sample_1 = \"The movie was overly long and the plot was confusing.\"\n",
    "sample_2 = \"An absolute masterpiece that I would watch again in a heartbeat.\"\n",
    "sample_3 = \"It was okay, not great but not terrible either.\"\n",
    "\n",
    "print(f\"Input: '{sample_1}'\\n -> {predict(sample_1, student_model, teacher_tokenizer)}\")\n",
    "print(f\"\\nInput: '{sample_2}'\\n -> {predict(sample_2, student_model, teacher_tokenizer)}\")\n",
    "print(f\"\\nInput: '{sample_3}'\\n -> {predict(sample_3, student_model, teacher_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 4: Evaluation and Benchmarking",
    "",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 15px; border-radius: 8px; color: white; margin-bottom: 20px;\">",
    "<h3 style=\"color: white; margin-top: 0; font-size: 1.2em; font-weight: 600;\">Benchmarking Function</h3>",
    "<p style=\"margin-bottom: 0; color: white; font-size: 0.95em;\">Compares multiple model variants across key metrics</p>",
    "</div>",
    "",
    "### üìè Metrics Measured:",
    "",
    "<div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 10px; margin: 20px 0;\">",
    "<div style=\"background: #e3f2fd; padding: 12px; border-radius: 5px; text-align: center;\">",
    "<div style=\"font-size: 1.2em; font-weight: bold;\">üìà</div>",
    "<div style=\"color: #212529; font-weight: 600;\">Accuracy</div>",
    "<div style=\"font-size: 0.85em; color: #666;\">Classification accuracy</div>",
    "</div>",
    "<div style=\"background: #f3e5f5; padding: 12px; border-radius: 5px; text-align: center;\">",
    "<div style=\"font-size: 1.2em; font-weight: bold;\">üíæ</div>",
    "<div style=\"color: #212529; font-weight: 600;\">Model Size</div>",
    "<div style=\"font-size: 0.85em; color: #666;\">Disk size in MB</div>",
    "</div>",
    "<div style=\"background: #fff3e0; padding: 12px; border-radius: 5px; text-align: center;\">",
    "<div style=\"font-size: 1.2em; font-weight: bold;\">‚ö°</div>",
    "<div style=\"color: #212529; font-weight: 600;\">Latency</div>",
    "<div style=\"font-size: 0.85em; color: #666;\">Inference time (ms)</div>",
    "</div>",
    "</div>",
    "",
    "### üî¨ Models Evaluated:",
    "",
    "<div style=\"background-color: #f5f5f5; padding: 15px; border-radius: 8px; margin: 15px 0;\">",
    "<ol style=\"margin: 0; padding-left: 20px; color: #212529;\">",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">üë®‚Äçüè´ Teacher (BERT)</strong>: Baseline - original fine-tuned model</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">üë®‚Äçüéì Student (FP32)</strong>: Distilled model in full precision</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">‚ö° Student (INT8)</strong>: Quantized to 8-bit integers for CPU inference</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">üíæ Student (4-bit)</strong>: Quantized using BitsAndBytes for GPU memory efficiency</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">üî¨ DistilBERT (Raw)</strong>: Untrained baseline (should be ~50% accuracy)</li>",
    "</ol>",
    "</div>",
    "",
    "<div style=\"background-color: #fff3cd; border-left: 4px solid #ffc107; padding: 12px; margin: 20px 0;\">",
    "<strong style=\"color: #856404;\">üîß Key Fix:</strong> <span style=\"color: #856404;\">DistilBERT doesn't use <code style=\"background: #fff; padding: 2px 6px; border-radius: 3px; color: #000;\">token_type_ids</code> (unlike BERT). Must remove this input to avoid errors.</span>",
    "</div>",
    "",
    "### üìä Expected Performance:",
    "",
    "<table style=\"width: 100%; border-collapse: collapse; margin: 20px 0;\">",
    "<tr style=\"background-color: #667eea; color: white;\">",
    "<th style=\"padding: 10px; text-align: left; color: white; font-weight: 600;\">Model</th>",
    "<th style=\"padding: 10px; text-align: center; color: white; font-weight: 600;\">Accuracy</th>",
    "<th style=\"padding: 10px; text-align: center; color: white; font-weight: 600;\">Size</th>",
    "<th style=\"padding: 10px; text-align: center; color: white; font-weight: 600;\">Speed</th>",
    "</tr>",
    "<tr style=\"background-color: #f5f5f5;\">",
    "<td style=\"padding: 10px; color: #212529;\"><strong>Teacher</strong></td>",
    "<td style=\"padding: 10px; text-align: center; color: #212529;\">~93%</td>",
    "<td style=\"padding: 10px; text-align: center; color: #212529;\">~440MB</td>",
    "<td style=\"padding: 10px; text-align: center; color: #212529;\">Baseline</td>",
    "</tr>",
    "<tr>",
    "<td style=\"padding: 10px; color: #212529;\"><strong>Student (FP32)</strong></td>",
    "<td style=\"padding: 10px; text-align: center; color: #212529;\">~90-92%</td>",
    "<td style=\"padding: 10px; text-align: center; color: #212529;\">~268MB</td>",
    "<td style=\"padding: 10px; text-align: center; color: #212529;\">~2x faster</td>",
    "</tr>",
    "<tr style=\"background-color: #f5f5f5;\">",
    "<td style=\"padding: 10px; color: #212529;\"><strong>Student (INT8)</strong></td>",
    "<td style=\"padding: 10px; text-align: center; color: #212529;\">Similar</td>",
    "<td style=\"padding: 10px; text-align: center; color: #212529;\">~67MB</td>",
    "<td style=\"padding: 10px; text-align: center; color: #212529;\">Fastest CPU</td>",
    "</tr>",
    "<tr>",
    "<td style=\"padding: 10px; color: #212529;\"><strong>Student (4-bit)</strong></td>",
    "<td style=\"padding: 10px; text-align: center; color: #212529;\">Similar</td>",
    "<td style=\"padding: 10px; text-align: center; color: #212529;\">~34MB</td>",
    "<td style=\"padding: 10px; text-align: center; color: #212529;\">Lowest VRAM</td>",
    "</tr>",
    "</table>",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goinXanNu-t1",
    "outputId": "9d9be8a3-dc45-4e25-9ea5-97fa95a94cf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Hardware: cuda\n",
      "\n",
      "Loading Teacher...\n",
      "\n",
      "--- Benchmarking: Teacher (BERT) (cuda) ---\n",
      "Evaluating 872 items........ Done.\n",
      "\n",
      "Loading Student (FP32)...\n",
      "\n",
      "--- Benchmarking: Student (FP32) (cuda) ---\n",
      "Evaluating 872 items........ Done.\n",
      "\n",
      "Quantizing Student (INT8)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2015461079.py:115: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  student_int8 = torch.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmarking: Student (INT8) (cpu) ---\n",
      "Evaluating 872 items........"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n",
      "\n",
      "Loading Student (4-bit)...\n",
      "Skipping 4-bit eval: bitsandbytes not installed.\n",
      "\n",
      "Loading Raw DistilBERT (Baseline)...\n",
      "\n",
      "--- Benchmarking: DistilBERT (Raw) (cuda) ---\n",
      "Evaluating 872 items........ Done.\n",
      "\n",
      "================================================================================\n",
      "Model                | Type   | Acc %  | Size MB  | Lat ms   | Notes\n",
      "--------------------------------------------------------------------------------\n",
      "Teacher (BERT)       | cuda   | 92.43  | 417.7    | 11.86    | Teacher\n",
      "Student (FP32)       | cuda   | 90.37  | 255.4    | 4.36     | Distilled Student\n",
      "Student (INT8)       | cpu    | 90.94  | 132.3    | 22.11    | Fastest CPU\n",
      "DistilBERT (Raw)     | cuda   | 49.08  | 255.4    | 4.29     | Untrained Base\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers datasets accelerate evaluate torch bitsandbytes scipy numpy\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP\n",
    "# ==========================================\n",
    "student_path = \"./distilled_student_saved\"  # Change this if your model is elsewhere\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Main Hardware: {device}\")\n",
    "\n",
    "# Check if model exists, otherwise warn user\n",
    "if not os.path.exists(student_path):\n",
    "    print(f\"WARNING: Path '{student_path}' not found. Using generic DistilBERT for demo.\")\n",
    "    student_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Load Data\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "val_dataset = dataset[\"validation\"]\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. BENCHMARK FUNCTION (With Bug Fix)\n",
    "# ==========================================\n",
    "def run_benchmark(model, name, device_type, dataset):\n",
    "    print(f\"\\n--- Benchmarking: {name} ({device_type}) ---\")\n",
    "\n",
    "    # A. Handle Device Placement\n",
    "    try:\n",
    "        if device_type == \"cuda\" and not hasattr(model, \"hf_device_map\"):\n",
    "            model.to(\"cuda\")\n",
    "        elif device_type == \"cpu\":\n",
    "            model.to(\"cpu\")\n",
    "    except:\n",
    "        pass # 4-bit models manage their own device map\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # B. Measure Size\n",
    "    try:\n",
    "        torch.save(model.state_dict(), \"temp.p\")\n",
    "        size_mb = os.path.getsize(\"temp.p\") / (1024 * 1024)\n",
    "        os.remove(\"temp.p\")\n",
    "    except:\n",
    "        size_mb = 0.0 # 4-bit models/quantized models often fail state_dict save\n",
    "\n",
    "    # C. Inference Loop\n",
    "    latencies = []\n",
    "    print(f\"Evaluating {len(dataset)} items...\", end=\"\")\n",
    "\n",
    "    for i, example in enumerate(dataset):\n",
    "        inputs = tokenizer(example[\"sentence\"], return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "\n",
    "        # --- CRITICAL FIX FOR DISTILBERT ---\n",
    "        if \"token_type_ids\" in inputs:\n",
    "            # Check strictly if model class contains 'DistilBert'\n",
    "            if \"DistilBert\" in type(model).__name__ or \"DistilBert\" in getattr(model.config, \"architectures\", [\"\"])[0]:\n",
    "                del inputs[\"token_type_ids\"]\n",
    "\n",
    "        # Move to device\n",
    "        if device_type == \"cuda\":\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "        # Timing\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        end = time.time()\n",
    "\n",
    "        latencies.append((end - start) * 1000)\n",
    "\n",
    "        # Accuracy\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        metric.add(prediction=preds, reference=example[\"label\"])\n",
    "\n",
    "        if i % 200 == 0: print(\".\", end=\"\")\n",
    "\n",
    "    final_acc = metric.compute()['accuracy'] * 100\n",
    "    avg_lat = np.mean(latencies)\n",
    "\n",
    "    print(\" Done.\")\n",
    "    return {\"Model\": name, \"Type\": device_type, \"Acc\": final_acc, \"Size\": size_mb, \"Lat\": avg_lat}\n",
    "\n",
    "# ==========================================\n",
    "# 3. RUN EVALUATIONS\n",
    "# ==========================================\n",
    "results_table = []\n",
    "\n",
    "# A. TEACHER (BERT) - Baseline\n",
    "print(\"\\nLoading Teacher...\")\n",
    "teacher = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "results_table.append(run_benchmark(teacher, \"Teacher (BERT)\", \"cuda\", val_dataset))\n",
    "del teacher\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# B. STUDENT (FP32) - Standard Distillation Result\n",
    "print(\"\\nLoading Student (FP32)...\")\n",
    "student_fp = AutoModelForSequenceClassification.from_pretrained(student_path)\n",
    "results_table.append(run_benchmark(student_fp, \"Student (FP32)\", \"cuda\", val_dataset))\n",
    "\n",
    "# C. STUDENT (INT8) - CPU Speed Optimized\n",
    "print(\"\\nQuantizing Student (INT8)...\")\n",
    "student_cpu = student_fp.to(\"cpu\")\n",
    "student_int8 = torch.quantization.quantize_dynamic(\n",
    "    student_cpu, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "results_table.append(run_benchmark(student_int8, \"Student (INT8)\", \"cpu\", val_dataset))\n",
    "del student_fp, student_int8\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# D. STUDENT (4-BIT) - VRAM Optimized (BitsAndBytes)\n",
    "print(\"\\nLoading Student (4-bit)...\")\n",
    "try:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    ) \n",
    "    student_4bit = AutoModelForSequenceClassification.from_pretrained(\n",
    "        student_path, quantization_config=bnb_config, device_map=\"auto\"\n",
    "    )\n",
    "    results_table.append(run_benchmark(student_4bit, \"Student (4-bit)\", \"cuda\", val_dataset))\n",
    "except ImportError:\n",
    "    print(\"Skipping 4-bit eval: bitsandbytes not installed.\")\n",
    "\n",
    "# E. BASELINE (Raw DistilBERT) - The Control Group\n",
    "# This model has NOT been fine-tuned on SST-2. Accuracy should be ~50%.\n",
    "print(\"\\nLoading Raw DistilBERT (Baseline)...\")\n",
    "raw_student = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "# We run this on CUDA to be fair to the other FP32 models\n",
    "results_table.append(run_benchmark(raw_student, \"DistilBERT (Raw)\", \"cuda\", val_dataset))\n",
    "del raw_student\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ==========================================\n",
    "# 4. FINAL TABLE\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'Model':<20} | {'Type':<6} | {'Acc %':<6} | {'Size MB':<8} | {'Lat ms':<8} | {'Notes'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in results_table:\n",
    "    note = \"\"\n",
    "    if \"4-bit\" in r['Model']: note = \"Low VRAM\"\n",
    "    elif \"INT8\" in r['Model']: note = \"Fastest CPU\"\n",
    "    elif \"Teacher\" in r['Model']: note = \"Teacher\"\n",
    "    elif \"Raw\" in r['Model']: note = \"Untrained Base\"\n",
    "    else: note = \"Distilled Student\"\n",
    "\n",
    "    print(f\"{r['Model']:<20} | {r['Type']:<6} | {r['Acc']:<6.2f} | {r['Size']:<8.1f} | {r['Lat']:<8.2f} | {note}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Step 5: Upload to Hugging Face Hub",
    "",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; color: white; margin: 20px 0;\">",
    "<h3 style=\"color: white; margin-top: 0; font-size: 1.3em; font-weight: 600;\">Model Deployment</h3>",
    "<p style=\"color: white; margin-bottom: 0;\">After successful distillation, upload the student model to Hugging Face Hub for:</p>",
    "</div>",
    "",
    "<div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0;\">",
    "<div style=\"background: #e3f2fd; padding: 15px; border-radius: 8px; border-top: 3px solid #2196f3;\">",
    "<h4 style=\"margin-top: 0; color: #1976d2; font-size: 1.1em; font-weight: 600;\">üîó Sharing</h4>",
    "<p style=\"margin-bottom: 0; color: #212529;\">Make model publicly available</p>",
    "</div>",
    "<div style=\"background: #f3e5f5; padding: 15px; border-radius: 8px; border-top: 3px solid #9c27b0;\">",
    "<h4 style=\"margin-top: 0; color: #7b1fa2; font-size: 1.1em; font-weight: 600;\">üìù Versioning</h4>",
    "<p style=\"margin-bottom: 0; color: #212529;\">Track model iterations</p>",
    "</div>",
    "<div style=\"background: #fff3e0; padding: 15px; border-radius: 8px; border-top: 3px solid #ff9800;\">",
    "<h4 style=\"margin-top: 0; color: #e65100; font-size: 1.1em; font-weight: 600;\">üîå Integration</h4>",
    "<p style=\"margin-bottom: 0; color: #212529;\">Easy loading with <code style=\"background: #f5f5f5; padding: 2px 6px; border-radius: 3px; color: #000;\">from_pretrained()</code></p>",
    "</div>",
    "<div style=\"background: #e8f5e9; padding: 15px; border-radius: 8px; border-top: 3px solid #4caf50;\">",
    "<h4 style=\"margin-top: 0; color: #388e3c; font-size: 1.1em; font-weight: 600;\">üìö Documentation</h4>",
    "<p style=\"margin-bottom: 0; color: #212529;\">Add model card with performance metrics</p>",
    "</div>",
    "</div>",
    "",
    "### üìã Upload Process:",
    "",
    "<div style=\"background-color: #f5f5f5; padding: 15px; border-radius: 8px; margin: 15px 0;\">",
    "<ol style=\"margin: 0; padding-left: 20px; color: #212529;\">",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">üîê Login</strong>: Authenticate with Hugging Face token</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">‚¨ÜÔ∏è Push</strong>: Upload model files (config.json, model weights, tokenizer)</li>",
    "<li style=\"color: #212529;\"><strong style=\"color: #000;\">‚úÖ Verify</strong>: Check model appears on your Hugging Face profile</li>",
    "</ol>",
    "</div>",
    "",
    "### üìù Model Card Best Practices:",
    "",
    "<div style=\"background-color: #d4edda; border-left: 4px solid #28a745; padding: 15px; margin: 20px 0;\">",
    "<ul style=\"margin: 0; padding-left: 20px; color: #155724;\">",
    "<li>Document distillation parameters (temperature, alpha)</li>",
    "<li>Report accuracy metrics (teacher vs student)</li>",
    "<li>Include inference benchmarks</li>",
    "<li>Note model size and speed improvements</li>",
    "<li>Specify use cases and limitations</li>",
    "</ul>",
    "</div>",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "754592d953744b6d88ebda9da855a971",
      "b5cfb11a07964fe99f8b084e5ba56215",
      "8e9e929f13a547e8b8b6b25d8a98f807",
      "91b028ae5114430480ccebdbfcf5d1f6",
      "34a49b39efdb409fa41f75b2b3f41eda",
      "e313176659684cc7aa615ac48ec69633",
      "8bb190f25166476f9215d74f5a6f5ef3",
      "4a4d4d99993544339f04481421cf7f0e",
      "493677c678774326827cfdd1033af867",
      "fcca44ed366645c4a2ae0efc9ad351a0",
      "469aaef517924b3f8db3228726262f39",
      "649fb5889af0432687d8c62f85595fec",
      "10370e0fb051413d97c702e006b9ee87",
      "756762f15ac141a19578cf6b20b7d70b",
      "7991890561f44a7b9249b1a9d18fb10d",
      "9e2fe13819854211b94aa25e3645210b",
      "9b45137a3485405cbd5bc2316cfbb5db",
      "ecf866efd6924eb9806480cbb41e89b6",
      "58b4bca6aa4745208c2c15d9f66f7dd2",
      "c05874081b984e7ba2e3553a811b8b49"
     ]
    },
    "id": "XBJDcuVKvbxU",
    "outputId": "727856c0-a191-4f61-bc38-dfad7359e873"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# This will create a widget where you paste your token\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235,
     "referenced_widgets": [
      "7456b866ce294c99a8b9a932f1dc68e5",
      "1d3335c1612442d3af9d28c6ce4fadc0",
      "8f90f93569294ff58f7b43b8a252be10",
      "574429d04aa6446fb1b595d345a147c7",
      "868297f00694454a85f3b8b20b05ec33",
      "f89f34f860374bdfb20b0414263b56b3",
      "a6fe11e7405c4390a13a94622c7f139f",
      "8e290f0d640b4f1084f4dd3252626fd5",
      "78fecd7ad9a64661a40351c564f5a42a",
      "cd1bbe48b70c428088655dcd6aef7c9c",
      "af98a5c8178544e990a0b0d56aeafde1",
      "9e03d2e65f9a4daeb88498c5e10a5ee4",
      "127ce984a2044e73951067439f1ad8a9",
      "04a609e2cdbe40bcac36944ca7c069d2",
      "c1c2aca812b04d0ca314b3adc4bc4791",
      "8fd854849b3245a4acb2b68f80ea5d25",
      "4ad6058248344e93ac640f389f92bb16",
      "aff680b7b07e4ecdb491ff4e77a2eb11",
      "3516358a2e5544fbb74a4c28e4e9f114",
      "ce757f98848c4414a7257dd0be45f90d",
      "a7a78a52578e4a3b9a4072b5698b6254",
      "2e65923d86a641fc9107789f444326b5",
      "3c12ebffd28d4c36a0a0dafad1ab404d",
      "9dee9030f40c479f8447bca9f837b073",
      "518f9ae0432e4d79a820b5d9c3fdc899",
      "a7621adea0aa44a69986d6da08dbf104",
      "d1d1a4c0991e47c5aeae77485bf697a1",
      "b41a6f0bfa4f4159ac0451e56b6d68cd",
      "660a0dd744f446eda2402c18dabf2b7b",
      "0c4346342e2946d98eef19cd2cbaad45",
      "2478b20a029e4360a51012f31e3da71e",
      "a3196979d0214107bf0ffc34d09e3908",
      "59e89d970b2a4f7f9a7580529d99fdb1",
      "5db88c83b7e7478faa76dd3d5b43f9d6",
      "3981b78236304ef8997f299c2abd6ff8",
      "58fa78e9c06b4fc9acd458a1e51e002e",
      "90f601ca9e2a48be841b8a3605ed209f",
      "ba7df223ab4a4d84973a16bca1be69ef",
      "c306994a8ee749709105ea219c0cf5c4",
      "56f855a1fbec4a53beed838ace52f78a",
      "f48b1f4ea35944b58050876420e4e371",
      "6dd44eb5609d4646b168c3d9ed3a9862",
      "983229c31deb4250b7b0d8caf6a47ad9",
      "c5d64fc28ede4f8cb2c78d5f1cce3ab3"
     ]
    },
    "id": "7hF9nWNYwOme",
    "outputId": "4ca0965f-0de3-4808-bf07-240e1062a17b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./distilled_student_saved...\n",
      "Pushing to Hugging Face Hub: distilbert-sst2-student...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success! Your model is live at:\n",
      "https://huggingface.co/Harsha901/distilbert-sst2-student\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# 1. Configuration\n",
    "# =====================================================\n",
    "local_model_path = \"./distilled_student_saved\"  # Where we saved it earlier\n",
    "repo_name = \"distilbert-sst2-student\"         # The name you want on the Hub\n",
    "username = \"Harsha901\"                   # OPTIONAL: It usually detects this auto-magically\n",
    "\n",
    "# 2. Load the Model & Tokenizer\n",
    "# =====================================================\n",
    "print(f\"Loading model from {local_model_path}...\")\n",
    "\n",
    "# Load the model (FP32 version is best for the Hub so others can quantize it themselves)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(local_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "# 3. Push to Hub\n",
    "# =====================================================\n",
    "print(f\"Pushing to Hugging Face Hub: {repo_name}...\")\n",
    "\n",
    "# This pushes the weights, config, and vocabulary\n",
    "# It will create the repo if it doesn't exist\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"\\nSuccess! Your model is live at:\")\n",
    "print(f\"https://huggingface.co/{username}/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSSX2d1W08P6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}